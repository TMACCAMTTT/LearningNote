\documentclass{ctexart}
\usepackage{ctex}
\usepackage{graphicx}

\title{kTree方法的改进方向}
\date{\today}

\begin{document}
	\maketitle
	在为所有训练样本学习到了最佳k值的基础上，给定一个测试样本，我们用kTree方法来快速搜索到该测试样本对应的训练样本子集。用该子集中的k值作为这个测试样本的最优k值。下面介绍kTree的建树，搜索过程，以及有待改进的方向。
	\section{加入启发式信息用于创建kTree}
	\subsection{决策树分类的过程（(基于ID3算法)}
	利用决策树进行分类分为两步：首先利用训练样本建立决策树模型，然后利用得到的决策树对测试样本分类。第一步的建模包括两个步骤：建树和剪枝。
	\subsubsection{建树}
	建树就是选取最优划分属性的过程。选择最优划分属性有多种方法，如ID3，C4.5，CART方法。
	
	ID3算法的划分准则是信息增益。我们用信息熵度量样本纯度，样本纯度越高，信息熵越低。称原样本的信息熵与用某个属性对样本划分后得到的信息熵的差值为信息增益，也就是样本纯度的提升程度。{\bfseries ID3算法每次选择信息增益最大的属性作为划分属性，这样做的一个缺点是偏向于选择属性取值更多的属性作为最优划分属性。}因为若该属性取值越多，这一次的划分就越细致，分类的不确定性就越低，于是信息增益越大。
	
	为了克服这点，提出C4.5算法。它在ID3的基础上提出了增益率作为划分准则，增益率是信息增益除以一个分母得到的值。这个分母与属性的取值个数有关，一般属性取值越多，该属性对应的这个分母值就越大。因此C4.5算法可以克服ID3算法的缺点。
	\subsubsection{剪枝}
	剪枝是为了处理过拟合，因为有时会把训练集自身的一些特点当作所有数据都具有的一般性质。{\bfseries 过度拟合一方面影响决策树的分类精度，另一方面会导致决策树规模过大。}剪枝分为预剪枝和后剪枝：
	
	预剪枝是对每个结点，在划分前先进行估计，若该结点的划分不能带来泛化性能的提升，则停止划分并将该结点标记为叶结点。预剪枝的优点：减少时间开销。缺点：可能导致欠拟合。因为某些分支的当前划分不能带来性能提升，但在其基础上进行的后续划分可能带来性能提升。
	
	后剪枝是先生成一棵完整的决策树，然后自底向上对非叶结点进行考察。若将该结点替换为叶结点能带来泛化性能的提升，则将该结点替换为叶节点。后剪枝的优点：通常比预剪枝保留了更多的分支，泛化性能更好。缺点：时间开销大。
	\subsection{仿照ID3算法建立kTree}
	我们仿照ID3算法来创建kTree：ID3算法用每个训练样本的类别作为其标签，而kTree用每个训练样本的最优k值作为其标签。{\bfseries 这样做的理论依据是：ID3算法的建树过程中，在各个分支结点上属性相同的训练样本被划分至同一个类别。于是这些样本间的距离应该较小，可以认为它们有相同或相近的k值。}在kTree中，我们认为这些样本有相同的k值。
	
	按上述方法建树之后，k值相同的训练样本被划分到同一个叶结点中。在测试过程中，给出一个测试样本，在kTree中找到它对应的叶结点，就将该叶结点对应的k值作为该测试样本的k值。
	\subsection{在建树过程中加入启发式信息}
	在ID3算法中，选择最优划分属性的唯一准则是信息增益。因为kTree是仿照ID3算法，它的划分准则也只有信息增益。于是我们设想，在划分准则中加入一些启发式信息，可能会创建出分类效果更好的决策树。
	
	这里的启发式信息有待研究。
	\section{如何体现所有样本}
	在kTree方法的建树过程中，为了防止过拟合，加入剪枝的步骤。因此不能体现所有样本。
	
	如何在kTree中体现所有样本，或者用一种新的数据结构来体现所有样本，有待研究。
	
\end{document}